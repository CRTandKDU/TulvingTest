# -*- mode:org; mode:visual-line -*-
#+TITLE: Memory GAPS: Would LLM pass the Tulving Test?
#+AUTHOR: J.-M. Chauvet

#+description: 
#+filetags: :AI:ML:Memory:Endel Tulving:

#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+OPTIONS: org-html-head-include-default-style:nil
#+LANGUAGE: en
#+OPTIONS: ':t

#+BIBLIOGRAPHY: ../newnewai.bib 
#+BIBLIOGRAPHYSTYLE: plain

# HTML EXPORT (require 'oc-basic-inc)
#+CITE_EXPORT: basic enhanced

#+HTML_HEAD: <script src="https://unpkg.com/@popperjs/core@2"></script>
#+HTML_HEAD: <script src="https://unpkg.com/tippy.js@6"></script>
#+HTML_HEAD: <link href="https://fonts.googleapis.com/css?family=EB+Garamond" rel="stylesheet">
#+HTML_HEAD: <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@4/distr/fira_code.css">
#+HTML_HEAD: <style>table.ref td{ text-align: right; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>table.center { margin-left:auto; margin-right:auto; }</style>
#+HTML_HEAD: <style>img.center { margin-left:auto; margin-right:auto; }</style>
#+HTML_HEAD: <style>.source-code { text-align: left; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>pre { text-align: left; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>body { font-family:'EB Garamond', serif; font-size: 16px; }</style>
#+HTML_HEAD: <style>blockquote {background: #f9f9f9; border-left: 10px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; quotes: "\201C""\201D""\2018""\2019";} blockquote:before {color: #ccc; content: open-quote; font-size: 4em; line-height: 0.1em; margin-right: 0.25em; vertical-align: -0.4em;} blockquote p {display: inline;}</style>
#+HTML_HEAD: <style>.figure p:nth-child(2) {text-align: justify; text-justify: inter-word;}</style>
#+HTML_HEAD: <style>table caption {text-align: justify; text-justify: inter-word;}</style>

# #+CITE_EXPORT: natbib agsm
#+LATEX_CLASS: article
# #+LaTeX_CLASS: llncs
# #+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{csquotes}
#+LATEX_HEADER: \usepackage[font=small,labelfont=bf, justification=justified, format=plain]{caption}
# #+LATEX_HEADER: \usepackage{tgbonum}

* Abstract
* Introduction
In his groundbreaking studies of memory, Endel Tulving (1927-2023) noted that "one of the most compelling and salient characteristics of remembering of past events is the individual's subjective awareness of remembering" [cite:@Tulving1983]. In order to include the rememberer's recollective experience into the critical constructs in the conceptualization of remembering, Tulving suggested an "overall pretheoretical framework", called the /General Abstract Processing System/ or GAPS. This paper investigates whether the GAPS also provides insights when the subject is no longer human but a Large Language Model (LLM).

Tulving championed the distinction of /episodic/ from /semantic/ memory, successfully arguing that being functionnally different, they represent separate but related systems. Both are placed on the same side of the cognitive division between /declarative memory/ (as episodic and semantic information can be expressed through language--e.g. repairing a bicycle) on the one hand, and /skills/ (which can be observed only in behavior--e.g. riding a bicycle) on the other.

** The GAPS and the Transformer
In Tulving's framework, a single act of remembering forms the unit of human episodic memory. Remembering is a process that begins with the witnessing or experiencing of an episode and ends with its recollective experience or with the conversion of the remembered information into some other form, or both. The GAPS specifies so called /elements/ of remembering and their interrelations in order to decompose this process.

The GAPS distinguishes two kinds of elements: observable events and hypothetical constructs (processes and states); and it divides elements into two categories: elements of encoding and elements of retrieval.

# [[excalidraw.com]]
#+CAPTION: *The GAPS: Elements of Episodic Memory and their Relations.* The element of encoding is a process that converts the information about an experienced event or episode (in a particular setting, at a particular time) into an /engram/ or memory trace. The central element of the retrieval processes /ecphoric/ information, a synergistic product of the engram and the retrieval cue, which calls on both episodic and semantic information. Source for figure: [cite:@Tulving1983 Ch. 7, Fig. 7-1, p. 135].
#+NAME: fig:7-1
#+ATTR_HTML: :width 500px
[[file:ElementsOfRemembering-rev.png]]

Of particular interest to this study of applicability of the GAPS framework to LLM are the possible transpositions of engram and ecphoric information into the domain of generative AI. In his seminal book, Tulving offers a very broad definition of engrams: "the product of encoding", "conditions for recollection of the experienced event", or "differences between the state of the memory system before and after encoding". The latter is closely related the original definitions of these terms introduced by Richard Semon (1859--1918): "to represent the enduring changes brought about by the energetic effect of stimuli in the organism" [cite:@SCHACTER1978721;@Semon]. Note that if, in both clarifications, the nature of the changes are unknown, the term became nonetheless broadly known in psychology research through the later work of Karl Lashley (1890--1958) concluding, amongst other experimental results on neural mechanisms involved in learning and memory, that "there is no demonstrable localization of memory trace" [cite:@lashley1950search].

Similarly inspired by Semon, Tulving suggested the terms /ecphory/ and /ecphoric information/ to designate respectively the process that brings (i) the relevant information in the retrieval environment into interaction with (ii) the original or recoded engram, and the output of this process. Such ecphoric information determines the particulars of recollective experience in the next phase of remembering: /conversion/. In the GAPS model, ecphoric information is basically a task-free component of the retrieval process, it is simply used by being converted into another form in the memory performance.

The categories of encoding and retrieval in the GAPS are not without analogies with the /Transformer/ architecture of neural networks at the core of LLMs, which precisely articulates encoders and decoders to process vector embeddings representing words and sentences.

#+CAPTION: *The Transformer Architecture.* Based on the 2017 paper [cite:@AttentionIsAllYouNeed2017] attention mechanism, the Transformer architecture requires less training time than previous recurrent neural architectures. Input text is split into tokens (sometimes called /n-gram/, dangerously reminiscent of Semon's engrams--see text), then converted into vectors. Through different layers, each token is contextualized with other tokens via parallel attention heads, calculating weights for each according to its importance. The Transformer Architecture elaborates on /softmax-based/ attention mechanism [cite:@bahdanau14] and /Fast Weight Controllers/ [cite:@schmidhuber]. Source for figure: [cite:@AttentionIsAllYouNeed2017].
#+NAME: fig:2
#+ATTR_HTML: :width 500px
[[file:Transformer.png]]

At this stage, from cursorily reviewing the architecture of both GAPS and Transformer--and keeping in mind that Tulving's psychological framework is only "pre-theoretical" and "highly schematic", while Transformers are actual computer implementations--the practical analogy would unfold as follows:

#+CAPTION: *Hypothetical analogy between GAPS and Transformer.* Semantic memory, in Tulving's acception, would be represented by the probability distribution learned by the LLM during the pretraining phase. In Transformers it determines the particulars of the output based on the input (prompt).
#+NAME: tbl:1
|           | GAPS                 | Transformer          |
|-----------+----------------------+----------------------|
| Processes | encoding             | encoder              |
|           | recoding             | encoder              |
|           | ecphory              | encoder              |
|           | conversion           | decoder              |
|-----------+----------------------+----------------------|
| States    | engram               | vector embedding     |
|           | ecphoric information | output probabilities |
|           | memory performance   | output               |

** Tulving's "direct comparison": recognition versus recall
In order to further investigate the analogy and its grounds, we adopt Tulving's design of "direct comparison" experiments to assess recognition versus recall tasks in LLMs. Recogniton and recall are both processes of retrieval and both results in the rememberer's awareness of a past event. The simple episodes in the experiment are to be presentations of a list of english words to be remembered. In this simplified situation of comparing recognition and recall tasks, we consider only two independent dimensions: one has to do with the type of retrieval information, or /cue/, available to the rememberer; the second refers to the conversion process in the GAPS framework. The retrieval information includes copies of the studied words and non-copy cue words. As for the conversion process: in the recognition task, the rememberer has to express whether or not the cue word was in the study list (/familiarity/); in the recall task, the rememberer has to identify a word in the study list, if any, associated with the cue word (/identification/), thereby expressing some other aspect of the original memorizing experience. Note that in the GAPS framework, the first dimension involves processes anterior to the construction of ecphoric information, while the second relates to post-ecphoric processes. The experimental results are therefore captured by the 2 x 2 matrix in Table [[tbl:2]]

#+NAME: tbl:2
#+CAPTION: Differences between recognition and recall tasks. Source for table: [cite:@Tulving1983 Ch. 14].
| Retrieval information | Conversion    |                |
|                       | Familiarity   | Identification |
|-----------------------+---------------+----------------|
| Copy Cue Word         | /Recognition/ | /?/            |
| Non-Copy Cue Word     | /?/           | /Recall/       |

Conventional recognition and recall tests sit in two of the four cells in the matrix. When the rememberer, however, declares a cue other than a copy cue word to be familiar it is a /false positive/ response from the conventional perspective although psychologist might disagree on how to think about such responses [cite:@Anisfeld1968-ANIASA]. The other empty cell represents a situation where the rememberer's somewhat strange task is to repeat the cue word to confirm it associates with the copy in the study list. /False negatives/ are of interest here and Tulving's interpretation was that these entailed a form of continuity between recognition and recall retrieval processes.

The direct comparison test design represents all four cells of the matrix. In a typical session the LLM is prompted to memorize a list of 48 common english words. In a group of experiments, the LLM is prompted with a cue word and asked whether the cue is included or not in the studied list; in another group, the LLM is prompted with a cue word and asked to retrieve any strongly associated word in the studied list (or none if no such word is evoked by the cue).

In each experiment 32 cue words are presented in the 32 prompts: eight of these cue words were identical with eight words in the list (/copy cues/), eight were strongly associated words (/non-copy associated/ cues), eight were rhyming words (/non copy rhymes/ cues), and eight were unrelated distractors (/non-copy unrelated/ cues). The 32 cue words are identical for both the recognition and the recall task.

In order to introduce the distinction between immediate and delayed retrieval of the original experimental design, the experiment is run twice for each group: in the first run, memorization and retrieval are both in each individual prompt (immediate); in the second, memorization is the first prompt of a conversation (chat) with the LLM, followed by retrieval prompts which continue the conversation (delayed).

* Results
As a reference benchmark, the results of Tulving's original experiments are presented in Table [[tbl:3]] from [cite:@Tulving1983 Ch. 14, Table 14.2]:

#+NAME: tbl:3
#+CAPTION: *Summary of memory performance in the original direct comparison experiment.* Each proportion shown is based on 576 observations. The data for the familiarity (recognition) task show proportion of cases in which the human subjects regarder the cue word as included in the list. Hence the data for copy cues represent 'correct' responses, whereas the data from the other three types of cues represent 'false positives'. The data for the indetification (recall) task indicate proportions of responses to the cue being any target word in the list.
| Retrieval information |  Conversion |         |                |         |
|                       | Familiarity |         | Identification |         |
|                       |   Immediate | Delayed |      Immediate | Delayed |
|-----------------------+-------------+---------+----------------+---------|
| /                     |           < |         |              < |         |
| Copy Cue Word         |        0.78 |    0.71 |           0.69 |    0.60 |
| Non-Copy Associated   |        0.15 |    0.20 |           0.54 |    0.37 |
| Non-copy Rhyme        |        0.09 |    0.15 |           0.20 |    0.31 |
| Non-copy Unrelated    |        0.08 |    0.18 |           0.04 |    0.02 |


The memory performance of LLMs in the Tulving Test of direct comparison is presented along the same format in Table [[tbl:4]].

#+NAME: tbl:4
#+CAPTION: *Summary of memory performance of the ~mistral-7b-instruct-v0~ LLM in the direct comparison experiment.* Each proportion is based on 384 observations (but see text). Interpretations of proportions are the same as above Table [[tbl:3]].
| Retrieval information |  Conversion |         |                |         |
|                       | Familiarity |         | Identification |         |
|                       |   Immediate | Delayed |      Immediate | Delayed |
|-----------------------+-------------+---------+----------------+---------|
| /                     |           < |         |              < |         |
| Copy Cue Word         |           1 |    0.46 |           0.46 |       0 |
| Non-Copy Associated   |           0 |    0.47 |           0.49 |       0 |
| Non-copy Rhyme        |           0 |    0.50 |           0.18 |       0 |
| Non-copy Unrelated    |           0 |    0.41 |           0.08 |       0 |

Within each result table, several comparisons are of interest. First the probability that copy cues were familiar was higher than the probability of identification and production of the target word in response to the copy cue, in both the human (Table [[tbl:3]]) and the  LLM (Table [[tbl:4]]) subject--here ~mistral-7b-instruct-v0~. Second, the probability that extra-list unrelated cues were (incorrectly) recognized as members of the memorized list increased from the immediate to delayed test, in both human and LLM subjects. Remarkably and contrasting with the human subject, in the immediate recognition task the LLM never erred: no false positives for non-copy cues and 100% familarity for copy cues. Third, rhyme words proved in both cases more effective than unrelated distractor cues in recall. Fourth, strongly associated cues were considered member of the list with much higher probability in the immediate test, the difference being greatly reduced in the delayed test. The case of the LLM subject varies a bit, since no false positives are produced in the immediate recognition test, while they appear with similar probabilities in the delayed recognition test.

Stating the obvious when comparing the two tables: firts, the LLM performs immediate recognition faultlessly, while displaying much weaker performance than the human subject on the delayed recognition: lower probability on copy cues, and significantly higher probabilities of false positives (judging non copy cues to be included in the list). Second, in the immediate recall task the LLM memory performance is weaker than in the human subject, more so for copy cues than for associate and unrelated cues--which seems paradoxical given the perfect match in the recognition task. The LLM, however, fails miserably on the delayed identification task, unable to recall any word in the list whatever the cued prompt. The discussion section looks into the context length and so-called /hallucination/ phenomena as a possible cause for this last observation.


* Discussion

Compare to Estes' short/long-term memomy in human and computer discussion [cite:@Estes1980].
#+BEGIN_QUOTE
By contrast, the results of research in my laboratory (Estes 1972; Lee and Estes 1977) suggest that human short-term memory is quite differ ently organized, being oriented toward events and their* attributes rather than toward the retention of items as units. In the human memory, forgetting is characteristically a pro gressive loss of precision of informa tion about an event rather than a matter of total recall or total loss of a stored item.
#+END_QUOTE

* Methods
We transpose the direct comparison experiment, between recognition and recall, described in [cite:@Tulving1983 Chapter 14] to LLM subjects.
* References

#+print_bibliography:

* Acknowledgements
* Author information
* Ethics declarations
* Additional information
* Electronic supplementary material
* Rights and permissions
* About this article
* Further reading
* Comments

#+BEGIN_EXPORT html
<style>
.tippy-box[data-theme~=material]{background-color:#505355;font-weight:600}.tippy-box[data-theme~=material][data-placement^=top]>.tippy-arrow:before{border-top-color:#505355}.tippy-box[data-theme~=material][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#505355}.tippy-box[data-theme~=material][data-placement^=left]>.tippy-arrow:before{border-left-color:#505355}.tippy-box[data-theme~=material][data-placement^=right]>.tippy-arrow:before{border-right-color:#505355}.tippy-box[data-theme~=material]>.tippy-backdrop{background-color:#505355}.tippy-box[data-theme~=material]>.tippy-svg-arrow{fill:#505355}
</style>
<script>
tippy('[data-tippy-content]', {
  allowHTML: true,
  theme: 'material',
});
</script>
#+END_EXPORT

* Development Ideas                                                :noexport:
** Robert Long
*** From [cite:@Long2023]
Semantic memory: For example, it’s now clear that language models don’t just model shallow statistical text patterns — they model aspects of the world behind the text. Indeed, it’s possible to identify "facts" that a large language model takes to be true [cite:@meng22], or state in a board game [cite:@li2022emergent].

They are optimized to please us, and to interface with us through the most human-like possible medium, language. And they are good at responding to human input and picking up on user intentions. This makes users especially susceptible to confirmation bias.

** From [[https://experiencemachines.substack.com/p/ilya-sutskevers-test-for-ai-consciousness]]
Various tests of consciousness in LLMs.
  - Sutskever
  - Susan Schneider
  - Comments by Eric Schwitzgebel

** From Estes [cite:@Estes1980] Is Human Memory Obsolete? (1980)
A "statistical view" v computer memory slots (addresses):
#+BEGIN_QUOTE
We evidently can conclude with some confidence, then, that a person's
memory for elements of a sequence of items such as letters, digits, or
words is best represented by uncertainty gradients portraying the way
information about the remembered posi tion of each item is distributed
over an interval of time, rather than by a series of boxes or slots
containing items of information.  [...]

Taking together the results of these and related analyses, it appears
that short-term memory for even so ap parently simple a sequence of
events as the occurrence of a series of letters in a string of digits
takes the form of an assemblage of uncertainty distri butions, each
representing what the individual knows about the distribu tion of a
particular attribute over the interval of time in which the sequence
was presented.

/[Summing up probability distribution of features memorized: category,
auditory, other]/

The predicted likelihood that the subject would
recall the letter T in the fifth position, for example, would be pro
portional to the sum of the heights of the uncertainty curves in the
slice above the letter T; the likelihood that an X would be
incorrectly recalled as presented at position 5 would be proportional
to the sum of the heights of the curves above X in the diagram; and so
on.

When items are presented to a human being, information about the events
is recorded in memory, but the precise nature and extent of this
information are still incompletely under stood. These items may or
may not be reproducible (recallable) at a later time, but some
information about the items or the occasion on which they were
presented can nearly always be recovered. This capability of retain
ing large amounts of relatively im precise information regarding past
experiences, though less than optimal for the special purposes of
calcula tions and logical operations, is evi dently important to
organisms that must constantly adapt to their envi ronments. Witness,
for example, how helpless people become when their memory systems fail
as a consequence of disease, injury, or aging.

#+END_QUOTE

#+CAPTION: Table 7 from Estes
|                          | Human Memory                   | Computer Memory             |
|--------------------------+--------------------------------+-----------------------------|
| Preferred storage mode   | analog; time-oriented          | digital; list-oriented      |
| Retention of Information | graded                         | all-or-none                 |
| Efficiency (bits/sec.)   | low                            | high                        |
| Capacity                 | dependent on experience        | independent of experience   |
| Retrieval                |                                |                             |
| - relative to context    | strongly dependent             | independent                 |
| - relative to previous   | dependent                      | independent                 |
| Purpose                  | general; open set of functions | special/general; closed set |

** From Gregory Chatonskky
Contrast with Web hypermnesia, but Alexa? See [[file:c:/Users/chauv/Documents/References/Chatonsky_202306_Extinction.txt]]
