# -*- mode:org; mode:visual-line -*-
#+TITLE: Memory GAPS: Are LLMs Tulving-Complete?
#+AUTHOR: J.-M. Chauvet

#+description: 
#+filetags: :AI:ML:Memory:Endel Tulving:

#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+OPTIONS: org-html-head-include-default-style:nil

#+BIBLIOGRAPHY: ../newnewai.bib 
#+BIBLIOGRAPHYSTYLE: plain

# HTML EXPORT (require 'oc-basic-inc)
#+CITE_EXPORT: basic enhanced

#+HTML_HEAD: <script src="https://unpkg.com/@popperjs/core@2"></script>
#+HTML_HEAD: <script src="https://unpkg.com/tippy.js@6"></script>
#+HTML_HEAD: <link href="https://fonts.googleapis.com/css?family=EB+Garamond" rel="stylesheet">
#+HTML_HEAD: <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@4/distr/fira_code.css">
#+HTML_HEAD: <style>table.ref td{ text-align: right; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>table.center { margin-left:auto; margin-right:auto; }</style>
#+HTML_HEAD: <style>img.center { margin-left:auto; margin-right:auto; }</style>
#+HTML_HEAD: <style>.source-code { text-align: left; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>pre { text-align: left; font-size: small; font-family:'Fira Code', monospace; }</style>
#+HTML_HEAD: <style>body { font-family:'EB Garamond', serif; font-size: 16px; }</style>
#+HTML_HEAD: <style>blockquote {background: #f9f9f9; border-left: 10px solid #ccc; margin: 1.5em 10px; padding: 0.5em 10px; quotes: "\201C""\201D""\2018""\2019";} blockquote:before {color: #ccc; content: open-quote; font-size: 4em; line-height: 0.1em; margin-right: 0.25em; vertical-align: -0.4em;} blockquote p {display: inline;}</style>
#+HTML_HEAD: <style>.figure p:nth-child(2) {text-align: justify; text-justify: inter-word;}</style>
#+HTML_HEAD: <style>table caption {text-align: justify; text-justify: inter-word;}</style>

# #+CITE_EXPORT: natbib agsm
#+LATEX_CLASS: article
# #+LaTeX_CLASS: llncs
# #+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lmodern}
# #+LATEX_HEADER: \usepackage{tgbonum}

* Abstract
* Introduction
In his groundbreaking studies of memory, Endel Tulving (1927-2023) noted that "one of the most compelling and salient characteristics of remembering of past events is the individual's subjective awareness of remembering" [cite:@Tulving1983]. In order to include the rememberer's recollective experience into the critical constructs in the conceptualization of remembering, Tulving suggested an "overall pretheoretical framework", called the /General Abstract Processing System/ or GAPS. This paper investigates whether the GAPS also provides insights when the subject is no longer human but a Large Language Model (LLM).

Tulving championed the distinction of /episodic/ from /semantic/ memory, successfully arguing that being functionnally different, they represent separate but related systems. Both are placed on the same side of the cognitive division between /declarative memory/ (as episodic and semantic information can be expressed through language--e.g. repairing a bicycle) on the one hand, and /skills/ (which can be observed only in behavior--e.g. riding a bicycle) on the other.

In Tulving's framework, a single act of remembering forms the unit of human episodic memory. Remembering is a process that begins with the witnessing or experiencing of an episode and ends with its recollective experience or with the conversion of the remembered information into some other form, or both. The GAPS specifies so called /elements/ of remembering and their interrelations in order to decompose this process.

The GAPS distinguishes two kinds of elements: observable events and hypothetical constructs (processes and states); and it divides elements into two categories: elements of encoding and elements of retrieval.

# [[excalidraw.com]]
#+ATTR_LATEX: :caption \captionsetup{justification=justified}
#+CAPTION: *The GAPS: Elements of Episodic Memory and their Relations.* The element of encoding is a process that converts the information about an experienced event or episode (in a particular setting, at a particular time) into an /engram/ or memory trace. The central element of the retrieval processes /ecphoric/ information, a synergistic product of the engram and the retrieval cue, which calls on both episodic and semantic information. Source for figure: [cite:@Tulving1983 Ch. 7, Fig. 7-1, p. 135].
#+NAME: fig:7-1
#+ATTR_HTML: :width 500px
[[file:ElementsOfRemembering-rev.png]]

Of particular interest to this study of applicability of the GAPS framework to LLM are the possible transpositions of engram and ecphoric information into the domain of generative AI. In his seminal book, Tulving offers a very broad definition of engrams: "the product of encoding", "conditions for recollection of the experienced event", or "differences between the state of the memory system before and after encoding". The latter is closely related the original definitions of these terms introduced by Richard Semon (1859--1918): "to represent the enduring changes brought about by the energetic effect of stimuli in the organism" [cite:@SCHACTER1978721;@Semon]. Note that if, in both clarifications, the nature of the changes are unknown, the term became nonetheless broadly known in psychology research through the later work of Karl Lashley (1890--1958) concluding, amongst other experimental results on neural mechanisms involved in learning and memory, that "there is no demonstrable localization of memory trace" [cite:@lashley1950search].

Similarly inspired by Semon, Tulving suggested the terms /ecphory/ and /ecphoric information/ to designate respectively the process that brings (i) the relevant information in the retrieval environment into interaction with (ii) the original or recoded engram, and the output of this process. Such ecphoric information determines the particulars of recollective experience in the next phase of remembering: /conversion/. In the GAPS model, ecphoric information is basically a task-free component of the retrieval process, it is simply used by being converted into another form in the memory performance.

The categories of encoding and retrieval in the GAPS are not without analogies with the /Transformer/ architecture of neural networks at the core of LLMs, which precisely articulates encoders and decoders to process vector embeddings representing words and sentences.

#+ATTR_LATEX: :caption \captionsetup{justification=justified}
#+CAPTION: *The Transformer Architecture.* Based on the 2017 paper [cite:@AttentionIsAllYouNeed2017] attention mechanism, the Transformer architecture requires less training time than previous recurrent neural architectures. Input text is split into tokens (sometimes called /n-gram/, dangerously reminiscent of Semon's engrams--see text), then converted into vectors. Through different layers, each token is contextualized with other tokens via parallel attention heads, calculating weights for each according to its importance. The Transformer Architecture elaborates on /softmax-based/ attention mechanism [cite:@bahdanau14] and /Fast Weight Controllers/ [cite:@schmidhuber]. Source for figure: [cite:@AttentionIsAllYouNeed2017].
#+NAME: fig:2
#+ATTR_HTML: :width 500px
[[file:Transformer.png]]

At this stage, from cursorily reviewing the architecture of both GAPS and Transformer--and keeping in mind that Tulving's psychological framework is only "pre-theoretical" and "highly schematic", while Transformers are actual computer implementations--the practical analogy would unfold as follows:

#+CAPTION: *Hypothetical analogy between GAPS and Transformer.* Semantic memory, in Tulving's acception, would be represented by the probability distribution learned by the LLM during the pretraining phase. In Transformers it determines the particulars of the output based on the input (prompt).
#+NAME: tbl:1
|           | GAPS                 | Transformer          |
|-----------+----------------------+----------------------|
| Processes | encoding             | encoder              |
|           | recoding             | encoder              |
|           | ecphory              | encoder              |
|           | conversion           | decoder              |
|-----------+----------------------+----------------------|
| States    | engram               | vector embedding     |
|           | ecphoric information | output probabilities |
|           | memory performance   | output               |

In order to further investigate the analogy and its grounds, we adopt Tulving's design of "direct comparison" experiments to assess recognition versus recall tasks in LLMs. Recogniton and recall are both processes of retrieval and both results in the rememberer's awareness of a past event. The simple episodes in the experiment are to be presentations of a list of english words to be remembered. In this simplified situation of comparing recognition and recall tasks, we consider only two independent dimensions: one has to do with the type of retrieval information, or /cue/, available to the rememberer; the second refers to the conversion process in the GAPS framework. The retrieval information includes copies of the studied words and non-copy cue words. As for the conversion process: in the recognition task, the rememberer has to express whether or not the cue word was in the study list (/familiarity/); in the recall task, the rememberer has to identify a word in the study list, if any, associated with the cue word (/identification/), thereby expressing some other aspect of the original memorizing experience. Note that in the GAPS framework, the first dimension involves processes anterior to the construction of ecphoric information, while the second relates to post-ecphoric processes. The experimental results are therefore captured by the 2 x 2 matrix in Table [[tbl:2]]

#+NAME: tbl:2
#+CAPTION: Differences between recognition and recall tasks. Source for table: [cite:@Tulving1983 Ch. 14].
| Retrieval information | Conversion    |                |
|                       | Familiarity   | Identification |
|-----------------------+---------------+----------------|
| Copy Cue Word         | /Recognition/ | ?              |
| Non-Copy Cue Word     | ?             | /Recall/       |

Conventional recognition and recall tests sit in two of the four cells in the matrix. When the rememberer, however, declares a cue other than a copy cue word to be familiar it is a /false positive/ response from the conventional perspective although psychologist might disagree on how to think about such responses [cite:@Anisfeld1968-ANIASA]. The other empty cell represents a situation where the rememberer's somewhat strange task is to repeat the cue word to confirm it associates with the copy in the study list. /False negatives/ are of interest here and Tulving's interpretation was of a form of continuity between recognition and recall retrieval processes.

* Results
* Discussion
Compare to Estes' short/long-term memomy in human and computer discussion [cite:@Estes1980].
#+BEGIN_QUOTE
By contrast, the results of research in my laboratory (Estes 1972; Lee and Estes 1977) suggest that human short-term memory is quite differ ently organized, being oriented toward events and their* attributes rather than toward the retention of items as units. In the human memory, forgetting is characteristically a pro gressive loss of precision of informa tion about an event rather than a matter of total recall or total loss of a stored item.
#+END_QUOTE

* Methods
We transpose the 'direct comparison' experiment, between recognitin and recall, described in [cite:@Tulving1983 Chapter 14] to LLM subjects.
* References

#+print_bibliography:

* Acknowledgements
* Author information
* Ethics declarations
* Additional information
* Electronic supplementary material
* Rights and permissions
* About this article
* Further reading
* Comments

#+BEGIN_EXPORT html
<style>
.tippy-box[data-theme~=material]{background-color:#505355;font-weight:600}.tippy-box[data-theme~=material][data-placement^=top]>.tippy-arrow:before{border-top-color:#505355}.tippy-box[data-theme~=material][data-placement^=bottom]>.tippy-arrow:before{border-bottom-color:#505355}.tippy-box[data-theme~=material][data-placement^=left]>.tippy-arrow:before{border-left-color:#505355}.tippy-box[data-theme~=material][data-placement^=right]>.tippy-arrow:before{border-right-color:#505355}.tippy-box[data-theme~=material]>.tippy-backdrop{background-color:#505355}.tippy-box[data-theme~=material]>.tippy-svg-arrow{fill:#505355}
</style>
<script>
tippy('[data-tippy-content]', {
  allowHTML: true,
  theme: 'material',
});
</script>
#+END_EXPORT

* Develop Ideas                                                    :noexport:
** Robert Long
*** From [cite:@Long2023]
Semantic memory: For example, it’s now clear that language models don’t just model shallow statistical text patterns — they model aspects of the world behind the text. Indeed, it’s possible to identify "facts" that a large language model takes to be true [cite:@meng22], or state in a board game [cite:@li2022emergent].

They are optimized to please us, and to interface with us through the most human-like possible medium, language. And they are good at responding to human input and picking up on user intentions. This makes users especially susceptible to confirmation bias.
*** From [[https://experiencemachines.substack.com/p/ilya-sutskevers-test-for-ai-consciousness]]
Various tests of consciousness in LLMs.
  - Sutskever
  - Susan Schneider
  - Comments by Eric Schwitzgebel


