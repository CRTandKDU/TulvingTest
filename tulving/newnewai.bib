@Comment Reminder: bibtex-fill-entry (bound to C-c C-q)
@Comment           M-x bibtex-reformat

@Comment Read on: Monday, February 19, 2024

@Article{Tulving_Watkins,
  author =	 {Tulving, Endel and Watkins, Michael J.},
  title =	 {{Structure Of Memory Traces}},
  journal =	 {Psychological Review},
  year =	 1975,
  volume =	 82,
  number =	 4,
  month =	 jul,
  pages =	 {261–275},
  issn =	 {0033-295X},
  doi =		 {10.1037/h0076782},
  url =		 {http://dx.doi.org/10.1037/h0076782},
  publisher =	 {American Psychological Association (APA)}
}

                  

@Comment Read on: Thursday, February 8, 2024

@article{Anisfeld1968-ANIASA,
  author =	 {Moshe Anisfeld, Margaret Knapp},
  doi =		 {10.1037/h0025782},
  journal =	 {{Journal of Experimental Psychology}},
  number =	 2,
  pages =	 171,
  title =	 {{Association, Synonymity, and Directionality in
                  False Recognition}},
  volume =	 77,
  year =	 1968
}

@ARTICLE{Estes1980,
  title =	 "Is human memory obsolete?",
  author =	 "Estes, W K",
  journal =	 "Am Sci",
  volume =	 68,
  number =	 1,
  pages =	 "62--69",
  month =	 jan,
  year =	 1980,
  address =	 "United States",
  language =	 "en"
}
                  

@Comment Read on: Monday, February 5, 2024
                  
@Online{meng22,
  author       = {Kevin Meng AND David Bau AND Alex Andonian AND
                  Yonatan Belinkov},
  title	       = {{Locating and Editing Factual Associations in GPT}},
  year	       = 2022,
  eprint       = {2202.05262v5},
  primaryclass = {cs.CL},
  archiveprefix= {arXiv}
}

@article{li2022emergent,
  author =	 {Li, Kenneth and Hopkins, Aspen K and Bau, David and
                  Vi{\'e}gas, Fernanda and Pfister, Hanspeter and
                  Wattenberg, Martin},
  title =	 {Emergent world representations: Exploring a sequence
                  model trained on a synthetic task},
  journal =	 {arXiv preprint arXiv:2210.13382},
  year =	 2022,
}

@Misc{Long2023,
  author =	 {Robert Long},
  title =	 {Are We Smart Enough to Know How Smart AIs Are?},
  howpublished = {Asterisk},
  month =	 6,
  year =	 2023,
  url =
                  {https://asteriskmag.com/issues/03/are-we-smart-enough-to-know-how-smart-ais-are}
}

@Comment Read on: Wednesday, January 24, 2024

@Online{arora23,
  author       = {Sanjeev Arora AND Anirudh Goyal},
  title	       = {{A Theory for Emergence of Complex Skills in
                  Language Models}},
  year	       = 2023,
  eprint       = {2307.15936v2},
  primaryclass = {cs.LG},
  archiveprefix= {arXiv}
}

                  

@Comment Read on: Monday, January 22, 2024

@Article{Buckner_2007,
  author =	 {Buckner, Randy L. and Carroll, Daniel C.},
  title =	 {Self-projection and the brain},
  journal =	 {Trends in Cognitive Sciences},
  year =	 2007,
  volume =	 11,
  number =	 2,
  month =	 feb,
  pages =	 {49–57},
  issn =	 {1364-6613},
  doi =		 {10.1016/j.tics.2006.11.004},
  url =		 {http://dx.doi.org/10.1016/j.tics.2006.11.004},
  publisher =	 {Elsevier BV}
}


@Comment Read on: Saturday, January 20, 2024

@InBook{Mitchell_2000,
  title =	 {Source Monitoring: Attributing Mental Experiences},
  year =	 2000,
  author =	 {Mitchell, Karen J and Johnson, Marcia K},
  booktitle =	 {The Oxford Handbook of Memory},
  publisher =	 {Oxford University PressNew York, NY},
  isbn =	 9780197736418,
  pages =	 {179–196},
  doi =		 {10.1093/oso/9780195122657.003.0012},
  url =
                  {http://dx.doi.org/10.1093/oso/9780195122657.003.0012},
  month =	 may
}



@Comment Read on: Wednesday, January 17, 2024

@techreport{smolensky2022neurocompositional,
  author =	 {Smolensky, Paul and McCoy, R. Thomas and Fernandez,
                  Roland and Goldrick, Matthew and Gao, Jianfeng},
  title =	 {{Neurocompositional computing in human and machine
                  intelligence: A tutorial}},
  institution =	 {Microsoft},
  year =	 2022,
  month =	 {May},
  abstract =	 {The past decade has produced a revolution in
                  Artificial Intelligence (AI), after a half-century
                  of AI repeatedly failing to meet expectations. What
                  explains the dramatic change from 20th-century to
                  21st-century AI, and how can remaining limitations
                  of current AI be overcome?  Until now, the widely
                  accepted narrative has attributed the recent
                  progress in AI to technical engineering advances
                  that have yielded massive increases in the quantity
                  of computational resources and training data
                  available to support statistical learning in deep
                  artificial neural networks. Although these
                  quantitative engineering innovations are important,
                  here we show that the latest advances in AI are not
                  solely due to quantitative increases in computing
                  power but also qualitative changes in how that
                  computing power is deployed. These qualitative
                  changes have brought about a new type of computing
                  that we call neurocompositional computing.  In
                  neurocompositional computing, neural networks
                  exploit two scientific principles that contemporary
                  theory in cognitive science maintains are
                  simultaneously necessary to enable human-level
                  cognition. The Compositionality Principle asserts
                  that encodings of complex information are structures
                  that are systematically composed from simpler
                  structured encodings. The Continuity Principle
                  states that the encoding and processing of
                  information is formalized with real numbers that
                  vary continuously.  These principles have seemed
                  irreconcilable until the recent mathematical
                  discovery that compositionality can be realized not
                  only through the traditional discrete methods of
                  symbolic computing, well developed in 20th-century
                  AI, but also through novel forms of continuous
                  neural computing—neurocompositional computing.  The
                  unprecedented progress of 21st-century AI has
                  resulted from the use of
                  limited—first-generation—forms of neurocompositional
                  computing. We show that the new techniques now being
                  deployed in second-generation neurocompositional
                  computing create AI systems that are not only more
                  robust and accurate than current systems, but also
                  more comprehensible—making it possible to diagnose
                  errors in, and exert human control over, artificial
                  neural networks through interpretation of their
                  internal states and direct intervention upon those
                  states.  Note: This tutorial is intended for those
                  new to this topic, and does not assume familiarity
                  with cognitive science, AI, or deep
                  learning. Appendices provide more advanced
                  material. Each figure, and the associated box
                  explaining it, provides an exposition, illustration,
                  or further details of a main point of the paper; in
                  order to make these figures relatively
                  self-contained, it has sometimes been necessary to
                  repeat some material from the text. For a brief
                  introduction and additional development of some of
                  this material see “Neurocompositional computing:
                  From the central paradox of cognition to a new
                  generation of ai systems” (arXiv:2205.01128; to
                  appear, AI Magazine)},
  url =
                  {https://www.microsoft.com/en-us/research/publication/neurocompositional-computing-in-human-and-machine-intelligence-a-tutorial/},
  number =	 {MSR-TR-2022-5},
  note =	 {52 pages main text, 78 pages total, 11 figures, 2
                  Appendices, 239 references. For a short presentation
                  of some of this material, see
                  https://arxiv.org/abs/2205.01128 (to appear in AI
                  Magazine).},
}

@Comment Read on: Sunday, January 14, 2024

@article{Stern1991-STEMOM,
  author =	 {David G. Stern},
  doi =		 {10.1080/09515089108573027},
  journal =	 {Philosophical Psychology},
  number =	 2,
  pages =	 {203--18},
  publisher =	 {Taylor \& Francis},
  title =	 {{Models of Memory: Wittgenstein and Cognitive
                  Science}},
  volume =	 4,
  year =	 1991
}

@article{squire2009memory,
  title =	 {{Memory and Brain Systems: 1969--2009}},
  author =	 {Squire, Larry R},
  journal =	 {{Journal of Neuroscience}},
  volume =	 29,
  number =	 41,
  pages =	 {12711--12716},
  year =	 2009,
  publisher =	 {Soc Neuroscience}
}
﻿

@Article{Robins2016,
  author =	 {Robins, Sarah},
  title =	 {{Representing the past: memory traces and the causal
                  theory of memory}},
  journal =	 {Philosophical Studies},
  year =	 2016,
  month =	 {Nov},
  day =		 01,
  volume =	 173,
  number =	 11,
  pages =	 {2993-3013},
  abstract =	 {{According to the Causal Theory of Memory (CTM),
                  remembering a particular past event requires a
                  causal connection between that event and its
                  subsequent representation in memory, specifically, a
                  connection sustained by a memory trace. The CTM is
                  the default view of memory in contemporary
                  philosophy, but debates persist over what the
                  involved memory traces must be like. Martin and
                  Deutscher (Philos Rev 75:161--196, 1966) argued that
                  the CTM required memory traces to be structural
                  analogues of past events. Bernecker (Memory: A
                  philosophical study. Oxford University Press,
                  Oxford, 2010) and Michaelian (Philos Psychol
                  24:323--342, 2011), contemporary CTM proponents,
                  reject structural analogues in favor of memory
                  traces as distributed patterns of event
                  features. The proposals are understood as distinct
                  accounts of how memory traces represent past
                  events. But there are two distinct questions one
                  could ask about a trace's representational
                  features. One might ask how memory traces, qua
                  mental representations, have their semantic
                  properties. Or, what makes memory traces, qua mental
                  representations of memories, distinct from other
                  mental representations. Proponents of the CTM, both
                  past and present, have failed to keep these two
                  questions distinct. The result is a serious but
                  unnoticed problem for the CTM in its current
                  form. Distributed memory traces are incompatible
                  with the CTM. Such traces do not provide a way to
                  track the causal history of individual memories, as
                  the CTM requires. If memory traces are distributed
                  patterns of event features, as Bernecker and
                  Michaelian each claim, then the CTM cannot be
                  right.}},
  issn =	 {1573-0883},
  doi =		 {10.1007/s11098-016-0647-x},
  url =		 {https://doi.org/10.1007/s11098-016-0647-x}
}

@Comment Read on: Tuesday, January 16, 2024

@article{Michaelian2011-MICGM,
  author =	 {Kourken Michaelian},
  doi =		 {10.1080/09515089.2011.559623},
  journal =	 {Philosophical Psychology},
  number =	 3,
  pages =	 {323--342},
  publisher =	 {Taylor \& Francis},
  title =	 {{Generative Memory}},
  volume =	 24,
  year =	 2011
}

@InCollection{sep-memory,
  author =	 {Michaelian, Kourken and Sutton, John},
  title =	 {{Memory}},
  booktitle =	 {The {Stanford} Encyclopedia of Philosophy},
  editor =	 {Edward N. Zalta},
  howpublished =
                  {\url{https://plato.stanford.edu/archives/sum2017/entries/memory/}},
  year =	 2017,
  edition =	 {{S}ummer 2017},
  publisher =	 {Metaphysics Research Lab, Stanford University}
}

@Comment Installed on: Friday, January 12, 2024

@Book{wordnet,
  title =	 {{WordNet: An Electronic Lexical Database}},
  author =	 {Christiane Fellbaum},
  year =	 1998,
  publisher =	 {Bradford Books},
  url =		 {https://mitpress.mit.edu/9780262561167/},
}

@Comment Read on: Dec 2023 - Jan 2024

@book{Tulving1983,
  author =	 {Tulving, Endel},
  publisher =	 {Oxford University Press},
  title =	 {{Elements of Episodic Memory}},
  year =	 1983
}

@article{lashley1950search,
  title =	 {{In Search of the Engram}},
  author =	 {Lashley, Karl S},
  year =	 1950,
  publisher =	 {Academic Press}
}

@Book{Semon,
  author =	 {Semon, Richard Wolfgang},
  title =	 {{Die Mneme als erhaltendes Prinzip im Wechsel des
                  organischen Geschehens}},
  year =	 1920,
  publisher =	 {Engelmann},
  doi =		 {10.5962/bhl.title.10234},
  url =		 {http://dx.doi.org/10.5962/bhl.title.10234}
}

@article{SCHACTER1978721,
  title =	 {{Richard Semon's Theory of Memory}},
  journal =	 {{Journal of Verbal Learning and Verbal Behavior}},
  volume =	 17,
  number =	 6,
  pages =	 {721-743},
  year =	 1978,
  issn =	 {0022-5371},
  doi =		 {https://doi.org/10.1016/S0022-5371(78)90443-7},
  url =
                  {https://www.sciencedirect.com/science/article/pii/S0022537178904437},
  author =	 {Schacter, Daniel L. AND Eich, James Eric AND Tulving,
                  Endel},
  abstract =	 {In the first decade of the 20th century, Richard
                  Semon put forward a theory of memory that
                  anticipated numerous recent developments in memory
                  research. The theory is discussed both in its
                  historical context and with reference to modern
                  ideas. Semon's theoretical concern for retrieval
                  phenomena is particularly noteworthy. Several
                  reasons are suggested why the theory is virtually
                  unknown today.}
}


@Comment Read on: Monday, January 8, 2024

@inproceedings{Winston2011TheSS,
  title =	 {{The Strong Story Hypothesis and the Directed
                  Perception Hypothesis}},
  author =	 {Patrick Henry Winston},
  booktitle =	 {AAAI Fall Symposium: Advances in Cognitive Systems},
  year =	 2011,
  url =		 {https://api.semanticscholar.org/CorpusID:15190640}
}

@Comment Read on: Tuesday, December 26, 2023﻿

@Article{MurphyRoyal2023,
  author =	 {Murphy-Royal, Ciaran and Ching, ShiNung and Papouin,
                  Thomas},
  title =	 {{A conceptual framework for astrocyte function}},
  journal =	 {Nature Neuroscience},
  year =	 2023,
  month =	 {Nov},
  day =		 01,
  volume =	 26,
  number =	 11,
  pages =	 {1848-1856},
  abstract =	 {The participation of astrocytes in brain computation
                  was hypothesized in 1992, coinciding with the
                  discovery that these cells display a form of
                  intracellular Ca2+ signaling sensitive to
                  neuroactive molecules. This finding fostered
                  conceptual leaps crystalized around the idea that
                  astrocytes, once thought to be passive, participate
                  actively in brain signaling and outputs. A multitude
                  of disparate roles of astrocytes has since emerged,
                  but their meaningful integration has been muddied by
                  the lack of consensus and models of how we conceive
                  the functional position of these cells in brain
                  circuitry. In this Perspective, we propose an
                  intuitive, data-driven and transferable conceptual
                  framework we coin contextual guidance. It describes
                  astrocytes as contextual gates that shape neural
                  circuitry in an adaptive, state-dependent
                  fashion. This paradigm provides fresh perspectives
                  on principles of astrocyte signaling and its
                  relevance to brain function, which could spur new
                  experimental avenues, including in computational
                  space.},
  issn =	 {1546-1726},
  doi =		 {10.1038/s41593-023-01448-8},
  url =		 {https://doi.org/10.1038/s41593-023-01448-8}
}

@Misc{Doctorow2023,
  author =	 {Cory Doctorow},
  title =	 {{What Kind of Bubble is AI?}},
  howpublished =
                  {\url{https://locusmag.com/2023/12/commentary-cory-doctorow-what-kind-of-bubble-is-ai/}},
  month =	 12,
  year =	 2023
}

@Comment Read on: Thursday, December 21, 2023

@ARTICLE{10.3389/fncel.2023.1220030,
  AUTHOR =	 {Gebicke-Haerter, Peter J.},
  TITLE =	 {The computational power of the human brain},
  JOURNAL =	 {Frontiers in Cellular Neuroscience},
  VOLUME =	 17,
  YEAR =	 2023,
  URL =
                  {https://www.frontiersin.org/articles/10.3389/fncel.2023.1220030},
  DOI =		 {10.3389/fncel.2023.1220030},
  ISSN =	 {1662-5102},
  ABSTRACT =	 {At the end of the 20th century, analog systems in
                  computer science have been widely replaced by
                  digital systems due to their higher computing
                  power. Nevertheless, the question keeps being
                  intriguing until now: is the brain analog or
                  digital? Initially, the latter has been favored,
                  considering it as a Turing machine that works like a
                  digital computer. However, more recently, digital
                  and analog processes have been combined to implant
                  human behavior in robots, endowing them with
                  artificial intelligence (AI). Therefore, we think it
                  is timely to compare mathematical models with the
                  biology of computation in the brain. To this end,
                  digital and analog processes clearly identified in
                  cellular and molecular interactions in the Central
                  Nervous System are highlighted. But above that, we
                  try to pinpoint reasons distinguishing in silico
                  computation from salient features of biological
                  computation. First, genuinely analog information
                  processing has been observed in electrical synapses
                  and through gap junctions, the latter both in
                  neurons and astrocytes. Apparently opposed to that,
                  neuronal action potentials (APs) or spikes represent
                  clearly digital events, like the yes/no or 1/0 of a
                  Turing machine. However, spikes are rarely uniform,
                  but can vary in amplitude and widths, which has
                  significant, differential effects on transmitter
                  release at the presynaptic terminal, where
                  notwithstanding the quantal (vesicular) release
                  itself is digital. Conversely, at the dendritic site
                  of the postsynaptic neuron, there are numerous
                  analog events of computation. Moreover, synaptic
                  transmission of information is not only neuronal,
                  but heavily influenced by astrocytes tightly
                  ensheathing the majority of synapses in brain
                  (tripartite synapse). At least at this point, LTP
                  and LTD modifying synaptic plasticity and believed
                  to induce short and long-term memory processes
                  including consolidation (equivalent to RAM and ROM
                  in electronic devices) have to be discussed. The
                  present knowledge of how the brain stores and
                  retrieves memories includes a variety of options
                  (e.g., neuronal network oscillations, engram cells,
                  astrocytic syncytium). Also epigenetic features play
                  crucial roles in memory formation and its
                  consolidation, which necessarily guides to molecular
                  events like gene transcription and translation. In
                  conclusion, brain computation is not only digital or
                  analog, or a combination of both, but encompasses
                  features in parallel, and of higher orders of
                  complexity.}
}

@article{smolensky_1988,
  title =	 {On the proper treatment of connectionism},
  volume =	 11,
  DOI =		 {10.1017/S0140525X00052432},
  number =	 1,
  journal =	 {Behavioral and Brain Sciences},
  publisher =	 {Cambridge University Press},
  author =	 {Smolensky, Paul},
  year =	 1988,
  pages =	 {1–23}
}

@article{https://doi.org/10.1111/j.1740-9713.2012.00590.x,
  author =	 {Norvig, Peter},
  title =	 {Colorless green ideas learn furiously: Chomsky and
                  the two cultures of statistical learning},
  journal =	 {Significance},
  volume =	 9,
  number =	 4,
  pages =	 {30-33},
  doi =		 {https://doi.org/10.1111/j.1740-9713.2012.00590.x},
  url =
                  {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2012.00590.x},
  eprint =
                  {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2012.00590.x},
  abstract =	 {Language recognition programs use massive databases
                  of words, and statistical correlations between those
                  words, to translate or to recognise speech. But
                  correlation is not causation. Do these statistical
                  data-dredgings give any insight into how language
                  works? Or are they a mere big-number trick, useful
                  but adding nothing to understanding? One who holds
                  the latter view is the theorist of language Noam
                  Chomsky. Peter Norvig disagrees.},
  year =	 2012
}

@inproceedings{kim-smolensky-2021-testing,
  title =	 "Testing for Grammatical Category Abstraction in
                  Neural Language Models",
  author =	 "Kim, Najoung and Smolensky, Paul",
  booktitle =	 "Proceedings of the Society for Computation in
                  Linguistics 2021",
  month =	 feb,
  year =	 2021,
  address =	 "Online",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://aclanthology.org/2021.scil-1.59",
  pages =	 "467--470",
}

@comment Neuroanatomy thread

@Article{pmid31492945,
  Author =	 "Buckner, R. L.  and DiNicola, L. M. ",
  Title =	 "{{T}he brain's default network: updated anatomy,
                  physiology and evolving insights}",
  Journal =	 "Nat Rev Neurosci",
  Year =	 2019,
  Volume =	 20,
  Number =	 10,
  Pages =	 "593--608",
  Month =	 "Oct",
  Abstract =	 {illness.}
}
@comment -----------------------------------------------------
@Comment Yann Le Cun (YLC) follow-up

@article{lecun2022path,
  title =	 {{A path towards autonomous machine intelligence
                  version 0.9. 2, 2022-06-27}},
  author =	 {LeCun, Yann},
  journal =	 {Open Review},
  volume =	 62,
  year =	 2022
}

@misc{dawid2023introduction,
  title =	 {Introduction to Latent Variable Energy-Based Models:
                  A Path Towards Autonomous Machine Intelligence},
  author =	 {Anna Dawid and Yann LeCun},
  year =	 2023,
  eprint =	 {2306.02572},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@Misc{Byrnes2023,
  key =		 {Byrnes2023},
  author =	 {Byrnes, Steven},
  title =	 {{LeCun’s “A Path Towards Autonomous Machine
                  Intelligence” has an unsolved technical alignment
                  problem}},
  howpublished =
                  {\url{https://www.lesswrong.com/posts/C5guLAx7ieQoowv3d/lecun-s-a-path-towards-autonomous-machine-intelligence-has-1}},
  month =	 {May},
  year =	 2023
}

@comment -----------------------------------------------------
@Comment LLM tasks v. problem-solving and goals

@misc{mahowald2023dissociating,
  title =	 {Dissociating language and thought in large language
                  models: a cognitive perspective},
  author =	 {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank
                  and Nancy Kanwisher and Joshua B. Tenenbaum and
                  Evelina Fedorenko},
  year =	 2023,
  eprint =	 {2301.06627},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@Article{pmid29341386,
  Author =	 "Coetzee, J. P.  and Monti, M. M. ",
  Title =	 "{{A}t the core of reasoning: {D}issociating
                  deductive and non-deductive load}",
  Journal =	 "Hum Brain Mapp",
  Year =	 2018,
  Volume =	 39,
  Number =	 4,
  Pages =	 "1850--1861",
  Month =	 "Apr"
}

@misc{valmeekam2023large,
  title =	 {Large Language Models Still Can't Plan (A Benchmark
                  for LLMs on Planning and Reasoning about Change)},
  author =	 {Karthik Valmeekam and Alberto Olmo and Sarath
                  Sreedharan and Subbarao Kambhampati},
  year =	 2023,
  eprint =	 {2206.10498},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{wei2023chainofthought,
  title =	 {Chain-of-Thought Prompting Elicits Reasoning in
                  Large Language Models},
  author =	 {Jason Wei and Xuezhi Wang and Dale Schuurmans and
                  Maarten Bosma and Brian Ichter and Fei Xia and Ed
                  Chi and Quoc Le and Denny Zhou},
  year =	 2023,
  eprint =	 {2201.11903},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{yao2023tree,
  title =	 {Tree of Thoughts: Deliberate Problem Solving with
                  Large Language Models},
  author =	 {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak
                  Shafran and Thomas L. Griffiths and Yuan Cao and
                  Karthik Narasimhan},
  year =	 2023,
  eprint =	 {2305.10601},
  archivePrefix ={arXiv},
  primaryClass = {cs.CL}
}

@misc{goyal2022coordination,
  title =	 {Coordination Among Neural Modules Through a Shared
                  Global Workspace},
  author =	 {Anirudh Goyal and Aniket Didolkar and Alex Lamb and
                  Kartikeya Badola and Nan Rosemary Ke and Nasim
                  Rahaman and Jonathan Binas and Charles Blundell and
                  Michael Mozer and Yoshua Bengio},
  year =	 2022,
  eprint =	 {2103.01197},
  archivePrefix ={arXiv},
  primaryClass = {cs.LG}
}

@misc{feichtenhofer2022masked,
  title =	 {Masked Autoencoders As Spatiotemporal Learners},
  author =	 {Christoph Feichtenhofer and Haoqi Fan and Yanghao Li
                  and Kaiming He},
  year =	 2022,
  eprint =	 {2205.09113},
  archivePrefix ={arXiv},
  primaryClass = {cs.CV}
}

@misc{bardes2022vicreg,
  title =	 {VICReg: Variance-Invariance-Covariance
                  Regularization for Self-Supervised Learning},
  author =	 {Adrien Bardes and Jean Ponce and Yann LeCun},
  year =	 2022,
  eprint =	 {2105.04906},
  archivePrefix ={arXiv},
  primaryClass = {cs.CV}
}

@misc{bardes2023mcjepa,
  title =	 {MC-JEPA: A Joint-Embedding Predictive Architecture
                  for Self-Supervised Learning of Motion and Content
                  Features},
  author =	 {Adrien Bardes and Jean Ponce and Yann LeCun},
  year =	 2023,
  eprint =	 {2307.12698},
  archivePrefix ={arXiv},
  primaryClass = {cs.CV}
}

@article{LKhc2020ContrastiveRL,
  title =	 {Contrastive Representation Learning: A Framework and
                  Review},
  author =	 {Ph{\'u}c H. L{\^e} Khắc and Graham Healy and Alan
                  F. Smeaton},
  journal =	 {IEEE Access},
  year =	 2020,
  volume =	 8,
  pages =	 {193907-193934},
  url =		 {https://api.semanticscholar.org/CorpusID:222291214}
}

@inproceedings{Toutanova2015RepresentingTF,
  title =	 {Representing Text for Joint Embedding of Text and
                  Knowledge Bases},
  author =	 {Kristina Toutanova and Danqi Chen and Patrick Pantel
                  and Hoifung Poon and Pallavi Choudhury and Michael
                  Gamon},
  booktitle =	 {Conference on Empirical Methods in Natural Language
                  Processing},
  year =	 2015,
  url =		 {https://api.semanticscholar.org/CorpusID:2127100}
}

@misc{assran2023selfsupervised,
  title =	 {Self-Supervised Learning from Images with a
                  Joint-Embedding Predictive Architecture},
  author =	 {Mahmoud Assran and Quentin Duval and Ishan Misra and
                  Piotr Bojanowski and Pascal Vincent and Michael
                  Rabbat and Yann LeCun and Nicolas Ballas},
  year =	 2023,
  eprint =	 {2301.08243},
  archivePrefix ={arXiv},
  primaryClass = {cs.CV}
}

@comment -----------------------------------------------------
@Comment Optimal control thread

@book{BrysonHo69,
  added-at =	 {2014-11-23T18:37:23.000+0100},
  address =	 {New York},
  author =	 {Bryson, A. E. and Ho, Y. C.},
  biburl =
                  {https://www.bibsonomy.org/bibtex/27d13cce517ce39632aeabf713bc6fbeb/prlz77},
  booktitle =	 {{Applied Optimal Control}},
  description =	 {CCNLab BibTeX},
  interhash =	 {3f3142a9345a8acc8c96f47a01da5372},
  intrahash =	 {7d13cce517ce39632aeabf713bc6fbeb},
  keywords =	 {backpropagation},
  publisher =	 {Blaisdell},
  timestamp =	 {2014-11-23T18:37:23.000+0100},
  title =	 {Applied Optimal Control},
  year =	 1969
}

@book{grimmett10,
  abstract =	 {{Grimmett's concise & masterful introduction to the
                  basic mathematical ideas needed to model such random
                  processes as viral marketing, epidemics, random
                  algorithms, and efficient routing. The selection of
                  topics & the approach taken to them is strongly
                  motivated by modern applications. Each chapter ends
                  with exciting exercises.}},
  added-at =	 {2016-06-07T11:56:02.000+0200},
  address =	 {Cambridge},
  author =	 {Grimmett, Geoffrey},
  biburl =
                  {https://www.bibsonomy.org/bibtex/29e715a5ddef351d178ef69531f23547a/ytyoun},
  interhash =	 {3b5234d0b29b18d9032757bdb438d454},
  intrahash =	 {9e715a5ddef351d178ef69531f23547a},
  isbn =	 {9780521197984 0521197988 9780521147354 0521147352},
  keywords =	 {circuit effective.resistance graph.theory laplacian
                  linear.algebra matrix monotonicity network
                  probability random.walk rayleigh resistor textbook},
  number =	 {Book 1},
  publisher =	 {Cambridge University Press},
  refid =	 607985577,
  series =	 {Institute of Mathematical Statistics Textbooks},
  timestamp =	 {2017-02-01T08:06:02.000+0100},
  title =	 {Probability on Graphs: Random Processes on Graphs
                  and Lattices},
  year =	 2010
}

@Comment -----------------------------------------------------
@Comment The persona thread

@unpublished{CerulloManuscript-CERIDO,
  year =	 2022,
  url =		 "https://philarchive.org/rec/CERIDO",
  author =	 {Michael Cerullo},
  title =	 {In Defense of Blake Lemoine and the Possibility of
                  Machine Sentience in Lamda}
}

@Comment -----------------------------------------------------
@Comment Clinical data

@inproceedings{mts-dialog,
  title =	 {An Empirical Study of Clinical Note Generation from
                  Doctor-Patient Encounters},
  author =	 "Ben Abacha, Asma and Yim, Wen-wai and Fan, Yadan and
                  Lin, Thomas",
  booktitle =	 "Proceedings of the 17th Conference of the European
                  Chapter of the Association for Computational
                  Linguistics",
  month =	 may,
  year =	 2023,
  address =	 "Dubrovnik, Croatia",
  publisher =	 "Association for Computational Linguistics",
  url =		 "https://aclanthology.org/2023.eacl-main.168",
  pages =	 "2291--2302"
}


@Online{schmidhuber,
  author       = {Imanol Schlag, Kazuki Irie, Jürgen
                  Schmidhuber},
  title	       = {{Linear Transformers Are Secretly Fast Weight
                  Programmers}},
  year	       = 2021,
  eprint       = {2102.11174v3},
  primaryclass = {cs.LG},
  archiveprefix= {arXiv}
}

@Online{bahdanau14,
  author       = {Dzmitry Bahdanau AND Kyunghyun Cho AND Yoshua
                  Bengio},
  title	       = {{Neural Machine Translation by Jointly Learning to
                  Align and Translate}},
  year	       = 2014,
  eprint       = {1409.0473v7},
  primaryclass = {cs.CL},
  archiveprefix= {arXiv}
}

@inproceedings{AttentionIsAllYouNeed2017,
  author =	 {Vaswani, Ashish AND Shazeer, Noam AND Parmar, Niki
                  AND Uszkoreit, Jakob AND Jones, Llion AND Gomez,
                  Aidan N., AND Kaiser, Łukasz AND Polosukhin, Illia},
  title =	 {Attention is all you need},
  year =	 2017,
  isbn =	 9781510860964,
  publisher =	 {Curran Associates Inc.},
  address =	 {Red Hook, NY, USA},
  abstract =	 {The dominant sequence transduction models are based
                  on complex recurrent or convolutional neural
                  networks that include an encoder and a decoder. The
                  best performing models also connect the encoder and
                  decoder through an attention mechanism. We propose a
                  new simple network architecture, the Transformer,
                  based solely on attention mechanisms, dispensing
                  with recurrence and convolutions
                  entirely. Experiments on two machine translation
                  tasks show these models to be superior in quality
                  while being more parallelizable and requiring
                  significantly less time to train. Our model achieves
                  28.4 BLEU on the WMT 2014 English-to-German
                  translation task, improving over the existing best
                  results, including ensembles, by over 2 BLEU. On the
                  WMT 2014 English-to-French translation task, our
                  model establishes a new single-model
                  state-of-the-art BLEU score of 41.0 after training
                  for 3.5 days on eight GPUs, a small fraction of the
                  training costs of the best models from the
                  literature.},
  booktitle =	 {{Proceedings of the 31st International Conference on
                  Neural Information Processing Systems}},
  pages =	 {6000–6010},
  numpages =	 11,
  location =	 {Long Beach, California, USA},
  series =	 {NIPS'17}
}

@Online{vaswani17:_atten_is_all_you_need,
  author       = {Ashish Vaswani, Noam Shazeer, Niki Parmar,
                  Jakob Uszkoreit, Llion Jones, Aidan N. Gomez
                 , Lukasz Kaiser, Illia Polosukhin},
  title	       = {{Attention Is All You Need}},
  year	       = 2017,
  eprint       = {1706.03762v7},
  primaryclass = {cs.CL},
  archiveprefix= {arXiv}
}

@book{nltk,
  title =	 {{Natural language processing with Python: analyzing
                  text with the Natural Language Toolkit}},
  author =	 {Bird, Steven and Klein, Ewan and Loper, Edward},
  year =	 2009,
  publisher =	 {O'Reilly Media, Inc.}
}

@Misc{CMUdict,
  author =	 {{Carnegie Mellon Speech Group}},
  title =	 {{The CMU Pronouncing Dictionary}},
  note =	 {\url{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}}
}

@Misc{llm:datasette,
  author =	 {Willison, Simon},
  title =	 {{LLM}},
  howpublished = {\url{https://llm.datasette.io/en/stable/index.html}},
  year =	 2023,
  note =	 {A CLI utility and Python library for interacting
                  with Large Language Models, both via remote APIs and
                  models that can be installed and run on your own
                  machine.}
}

@Online{jiang23:_mistr,
  author       = {Albert Q. Jiang AND Alexandre Sablayrolles,
                  Arthur Mensch AND Chris Bamford AND Devendra Singh
                  Chaplot AND Diego de las Casas AND Florian Bressand
                  AND Gianna Lengyel AND Guillaume Lample AND Lucile
                  Saulnier AND Lélio Renard Lavaud AND Marie-Anne
                  Lachaux AND Pierre Stock AND Teven Le Scao,
                  Thibaut Lavril AND Thomas Wang AND Timothée Lacroix
                  AND William El Sayed},
  title	       = {{Mistral 7B}},
  year	       = 2023,
  eprint       = {2310.06825v1},
  primaryclass = {cs.CL},
  archiveprefix= {arXiv}
}

@misc{orca_mini_3b,
  author =	 {Pankaj Mathur},
  title =	 {{An explain tuned OpenLLaMA-3b model on custom
                  wizardlm, alpaca, and dolly datasets}},
  year =	 2023,
  publisher =	 {GitHub, HuggingFace},
  journal =	 {GitHub repository, HuggingFace repository},
  howpublished =
                  {\url{https://github.com/pankajarm/wizardlm_alpaca_dolly_orca_open_llama_3b},
                  \url{https://https://huggingface.co/psmathur/wizardlm_alpaca_dolly_orca_open_llama_3b}},
}
